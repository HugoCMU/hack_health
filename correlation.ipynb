{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Hackathon: Health Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "ROOT_DIR = Path.cwd()\n",
    "STEPS_JSON = ROOT_DIR / 'steps.json'\n",
    "WEIGHT_JSON = ROOT_DIR / 'weight.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_data = pd.read_json(str(STEPS_JSON))\n",
    "weight_data = pd.read_json(str(WEIGHT_JSON))\n",
    "\n",
    "# Change weight data to have a consistent date category\n",
    "def change_date(create_time_string):\n",
    "    m = re.search('(\\d\\d\\d\\d-\\d\\d-\\d\\d)', str(create_time_string))\n",
    "    return m.group(1)\n",
    "\n",
    "weight_data['adate'] = weight_data['time'].apply(change_date)\n",
    "steps_data['adate'] = steps_data['date'].apply(change_date)\n",
    "\n",
    "# Join weight and steps data\n",
    "weight_data = weight_data.merge(steps_data, on='adate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have this data cleaned up as a CSV now\n",
    "GYRO_CSV = ROOT_DIR / 'Anand-history.csv'\n",
    "gyro = pd.read_csv(GYRO_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correlation matrix using Seaborn\n",
    "corrmat = gyro.corr() #weight_data.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Next Steps\n",
    "\n",
    "Try to predict future health information\n",
    "\n",
    "Sources:\n",
    " - [1] http://chandlerzuo.github.io/blog/2017/11/darnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "class da_rnn:\n",
    "    def __init__(self, dataframe,\n",
    "                 target,\n",
    "                 encoder_hidden_size = 64,\n",
    "                 decoder_hidden_size = 64,\n",
    "                 T = 10,\n",
    "                 learning_rate = 0.01,\n",
    "                 batch_size = 128,\n",
    "                 parallel = True,\n",
    "                 debug = False):\n",
    "        \n",
    "        self.T = T\n",
    "        self.dat = dataframe\n",
    "        \n",
    "        # Input data and target\n",
    "        self.X = dataframe.loc[:, [x for x in dat.columns.tolist() if x != target]].as_matrix()\n",
    "        self.y = np.array(dataframe[target])\n",
    "        self.y = self.y - np.mean(self.y[:self.train_size])\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_size = int(self.X.shape[0] * 0.7)\n",
    "        \n",
    "        print(\"Training size: %d.\", self.train_size)\n",
    "\n",
    "    def train(self, n_epochs = 10):\n",
    "        iter_per_epoch = int(np.ceil(self.train_size * 1. / self.batch_size))\n",
    "        logger.info(\"Iterations per epoch: %3.3f ~ %d.\", self.train_size * 1. / self.batch_size, iter_per_epoch)\n",
    "        self.iter_losses = np.zeros(n_epochs * iter_per_epoch)\n",
    "        self.epoch_losses = np.zeros(n_epochs)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "        n_iter = 0\n",
    "\n",
    "        learning_rate = 1.\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "            perm_idx = np.random.permutation(self.train_size - self.T)\n",
    "            j = 0\n",
    "            while j < self.train_size:\n",
    "                batch_idx = perm_idx[j:(j + self.batch_size)]\n",
    "                X = np.zeros((len(batch_idx), self.T - 1, self.X.shape[1]))\n",
    "                y_history = np.zeros((len(batch_idx), self.T - 1))\n",
    "                y_target = self.y[batch_idx + self.T]\n",
    "\n",
    "                for k in range(len(batch_idx)):\n",
    "                    X[k, :, :] = self.X[batch_idx[k] : (batch_idx[k] + self.T - 1), :]\n",
    "                    y_history[k, :] = self.y[batch_idx[k] : (batch_idx[k] + self.T - 1)]\n",
    "\n",
    "                loss = self.train_iteration(X, y_history, y_target)\n",
    "                self.iter_losses[i * iter_per_epoch + j / self.batch_size] = loss\n",
    "                #if (j / self.batch_size) % 50 == 0:\n",
    "                #    self.logger.info(\"Epoch %d, Batch %d: loss = %3.3f.\", i, j / self.batch_size, loss)\n",
    "                j += self.batch_size\n",
    "                n_iter += 1\n",
    "\n",
    "                if n_iter % 10000 == 0 and n_iter > 0:\n",
    "                    for param_group in self.encoder_optimizer.param_groups:\n",
    "                        param_group['lr'] = param_group['lr'] * 0.9\n",
    "                    for param_group in self.decoder_optimizer.param_groups:\n",
    "                        param_group['lr'] = param_group['lr'] * 0.9\n",
    "\n",
    "            self.epoch_losses[i] = np.mean(self.iter_losses[range(i * iter_per_epoch, (i + 1) * iter_per_epoch)])\n",
    "            if i % 10 == 0:\n",
    "                self.logger.info(\"Epoch %d, loss: %3.3f.\", i, self.epoch_losses[i])\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                y_train_pred = self.predict(on_train = True)\n",
    "                y_test_pred = self.predict(on_train = False)\n",
    "                y_pred = np.concatenate((y_train_pred, y_test_pred))\n",
    "                plt.figure()\n",
    "                plt.plot(range(1, 1 + len(self.y)), self.y, label = \"True\")\n",
    "                plt.plot(range(self.T , len(y_train_pred) + self.T), y_train_pred, label = 'Predicted - Train')\n",
    "                plt.plot(range(self.T + len(y_train_pred) , len(self.y) + 1), y_test_pred, label = 'Predicted - Test')\n",
    "                plt.legend(loc = 'upper left')\n",
    "                plt.show()\n",
    "\n",
    "    def train_iteration(self, X, y_history, y_target):\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        input_weighted, input_encoded = self.encoder(Variable(torch.from_numpy(X).type(torch.FloatTensor).cuda()))\n",
    "        y_pred = self.decoder(input_encoded, Variable(torch.from_numpy(y_history).type(torch.FloatTensor).cuda()))\n",
    "\n",
    "        y_true = Variable(torch.from_numpy(y_target).type(torch.FloatTensor).cuda())\n",
    "        loss = self.loss_func(y_pred, y_true)\n",
    "        loss.backward()\n",
    "\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "\n",
    "        return loss.data[0]\n",
    "\n",
    "    def predict(self, on_train = False):\n",
    "        if on_train:\n",
    "            y_pred = np.zeros(self.train_size - self.T + 1)\n",
    "        else:\n",
    "            y_pred = np.zeros(self.X.shape[0] - self.train_size)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(y_pred):\n",
    "            batch_idx = np.array(range(len(y_pred)))[i : (i + self.batch_size)]\n",
    "            X = np.zeros((len(batch_idx), self.T - 1, self.X.shape[1]))\n",
    "            y_history = np.zeros((len(batch_idx), self.T - 1))\n",
    "            for j in range(len(batch_idx)):\n",
    "                if on_train:\n",
    "                    X[j, :, :] = self.X[range(batch_idx[j], batch_idx[j] + self.T - 1), :]\n",
    "                    y_history[j, :] = self.y[range(batch_idx[j],  batch_idx[j]+ self.T - 1)]\n",
    "                else:\n",
    "                    X[j, :, :] = self.X[range(batch_idx[j] + self.train_size - self.T, batch_idx[j] + self.train_size - 1), :]\n",
    "                    y_history[j, :] = self.y[range(batch_idx[j] + self.train_size - self.T,  batch_idx[j]+ self.train_size - 1)]\n",
    "\n",
    "            y_history = Variable(torch.from_numpy(y_history).type(torch.FloatTensor).cuda())\n",
    "            _, input_encoded = self.encoder(Variable(torch.from_numpy(X).type(torch.FloatTensor).cuda()))\n",
    "            y_pred[i:(i + self.batch_size)] = self.decoder(input_encoded, y_history).cpu().data.numpy()[:, 0]\n",
    "            i += self.batch_size\n",
    "        return y_pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
